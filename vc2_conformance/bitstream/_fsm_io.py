r"""
A series of facilities for performing low-level IO operations on regular
repeating structures of bitstream values whose structure can be defined by a
Finite State Machine (FSM).

Provides functions which directly read/write from :py:class:`BitstreamReader`\
s and :py:class:`BitstreamWriter`\ s.

These functions are intended for the special-case operation of bulk
reading/writing (for example) coefficient values from a bitstream where the
higher-level :py:class:`BitstreamValue` structures have too much overhead due
to Python objects being constructed for every individual value in the stream.
By contrast, FSM-based I/O reads/writes integers directly to and from
pre-allocated Python lists.

All of the FSM I/O functions take a 'token generator' as an argument. This
argument should be a Python generator which produces :py:class:`Token` tuples
which define the data type of the next value to be read/written in the
bitstream.

Each :py:class:`Token` specifies a :py:class:`token <TokenTypes>`, an
``argument`` and a ``target``.

The ``type`` and ``argument`` values define what action is to take place. For
example, :py:data`TokenTypes.nbits` specifies that a fixed-width unsigned
integer should be read or written with the length specified by the ``argument``
field.

The ``target`` field of each :py:class:`Token` defines where read/written
values should be stored/retreived. The FSM I/O operations take a dictionary
which maps from target identifiers to flat (1D) python :py:class:`list`\ s.
Each time a token is emitted with a particular target, read/written values will
be stored/retreived from the corresponding list.

As a very simple example, the following generator function and target
dictionary might be used to read or write arrays of interleaved 8-bit colour
values from a bitstream::

    >>> def example_token_generator_1():
    ...     for _ in range(10):
    ...         yield Token(TokenTypes.nbits, 8, "r")
    ...         yield Token(TokenTypes.nbits, 8, "g")
    ...         yield Token(TokenTypes.nbits, 8, "b")
    >>> # Value arrays should be be pre-allocated
    >>> values = {"r": [0]*10,
    ...           "g": [0]*10,
    ...           "b": [0]*10}
    
    >>> # Read some values
    >>> read_fsm(reader, example_token_generator_1(), values)
    
    >>> # Write some values (in the same format)
    >>> write_fsm(writer, example_token_generator_1(), values)
    
    >>> # Compute the length of the sequence (in bits)
    >>> calculate_fsm_length(example_token_generator_1(), values)

In practice, sequences often change their behaviour based on previous values
encountered in the sequence. For example, a bounded block's size might defined
as part of the stream. As the FSM I/O functions step the token generator, they
send the generator the corresponding read/written values to allow it to react
to values as they pass::

    >>> def example_token_generator_2():
    ...     for _ in range(10):
    ...         length_bytes = yield Token(TokenTypes.nbits, 8, "length_bytes")
    ...         yield Token(TokenTypes.bounded_block_begin, length_bytes * 8, None)
    ...         for _ in range(100):
    ...             yield Token(TokenTypes.sint, None, "value")
    ...         yield Token(TokenTypes.bounded_block_end, None, "padding")
"""

from collections import namedtuple

from enum import Enum, auto

from vc2_conformance.bitstream import (
    BoundedReader,
    BoundedWriter,
)

from vc2_conformance.bitstream._integer_io import (
    read_bits,
    write_bits,
    exp_golomb_length,
    read_exp_golomb,
    write_exp_golomb,
    signed_exp_golomb_length,
    read_signed_exp_golomb,
    write_signed_exp_golomb,
)


Token = namedtuple("Token", "type, argument, target")
"""
A token to be generated by a token generator function. Defines a ``token`` (one
of :py:class:`TokenTypes`), an argument and a target.
"""


class TokenTypes(Enum):
    """
    Tokens which an FSM can emit to indicate the type of value which is
    expected to appear next in the bitstream.
    """
    
    nbits = auto()
    """
    The next value will be a fixed-length, unsigned number (as per (A.3.3)
    read_nbits()).
    
    Takes a length (in bits) as argument.
    
    Consumes/produces an int.
    """
    
    uint = auto()
    """
    A variable-length, unsigned number (as per (A.4.3) read_uint()).
    
    Takes no arguments.
    
    Consumes/produces an int.
    """
    
    sint = auto()
    """
    A variable-length, signed number (as per (A.4.4) read_sint()).
    
    Takes no arguments.
    
    Consumes/produces an int.
    """
    
    bounded_block_begin = auto()
    """
    Defines the start of a bounded block (as per (A.4.2)). Must be followed
    some time later by bounded_block_end.
    
    Takes the number of bits in the block as an argument.
    
    Does not consume or produce any value.
    """
    
    bounded_block_end = auto()
    """
    Defines the start of a bounded block (as per (A.4.2)).
    
    Takes no arguments.
    
    Consumes/produces an int containing the value of the unused bits in the
    block.
    """


def read_fsm(reader, token_generator, values):
    """
    Read a series of values according to the tokens generated by
    ``token_generator``.
    
    Parameters
    ==========
    reader : :py:class:`BitstreamReader`
    token_generator : generator
        A generator which yields :py:class:`Token` tuples defining what values
        are to be read from the bitstream. The generator will be sent the read
        values in return.
    values : {target_identifier: [value, ...], ...}
        A dictionary which, for each of the target identifiers generated by the
        ``token_generator``, contains a list pre-initialised to hold the number
        of values which will be read from the bitstream for that target. These
        lists will be mutated such that element i contains the value read the
        ith time that target was named by a :py:class:`Token`.

    Returns
    =======
    bits_past_eof : int
        The number of bits beyond the end of the file which were read.
    """
    bits_past_eof = 0
    
    # The index of the next value to be written into each target array
    next_value_index = {target_identifier: 0 for target_identifier in values}
    
    # Whenever a bounded block is started, the current reader and bits_past_eof
    # count is placed onto the reader_stack and bits_past_eof_stack
    # respectively. At the end of the bounded block, the old values are
    # restored.
    reader_stack = []
    bits_past_eof_stack = []
    try:
        # The latest value to have been read (to send back to the token
        # generator)
        latest_value = None
        while True:
            token = token_generator.send(latest_value)
            if token.type is TokenTypes.nbits:
                latest_value, this_bits_past_eof = read_bits(reader, token.argument)
                bits_past_eof += this_bits_past_eof
            elif token.type is TokenTypes.uint:
                latest_value, this_bits_past_eof = read_exp_golomb(reader)
                bits_past_eof += this_bits_past_eof
            elif token.type is TokenTypes.sint:
                latest_value, this_bits_past_eof = read_signed_exp_golomb(reader)
                bits_past_eof += this_bits_past_eof
            elif token.type is TokenTypes.bounded_block_begin:
                reader_stack.append(reader)
                reader = BoundedReader(reader, token.argument)
                
                bits_past_eof_stack.append(bits_past_eof)
                bits_past_eof = 0
                
                latest_value = None
            elif token.type is TokenTypes.bounded_block_end:
                bits_past_eof = bits_past_eof_stack.pop()
                bits_past_eof += reader.bits_past_eof
                
                # Read any unused bits
                unused_bits = reader.bits_remaining
                reader = reader_stack.pop()
                latest_value, this_bits_past_eof = read_bits(reader, unused_bits)
                bits_past_eof += this_bits_past_eof
            else:
                raise ValueError("Unrecognised TokenType: {}".format(token.type))
            
            if token.target is not None:
                values[token.target][next_value_index[token.target]] = latest_value
                next_value_index[token.target] += 1
    except StopIteration:
        pass
    finally:
        token_generator.close()
    
    # Check for un-closed bounded blocks
    if len(reader_stack) > 0:
        raise ValueError("Not every bounded_block_begin had a matching bounded_block_end")
    
    # Check all values were used
    for target_identifier in values:
        if next_value_index[target_identifier] != len(values[target_identifier]):
            raise ValueError("Targets {} not fully populated".format(
                target_identifier))
    
    return bits_past_eof


def write_fsm(writer, token_generator, values):
    """
    Write a series of values according to the tokens generated by
    ``token_generator``.
    
    Parameters
    ==========
    writer : :py:class:`BitstreamWriter`
    token_generator : generator
        A generator which yields :py:class:`Token` tuples defining what values
        are to be written to the bitstream. The generator will be sent the
        written values in return.
    values : {target_identifier: [value, ...], ...}
        A dictionary which, for each of the target identifiers generated by the
        ``token_generator``, contains the values to be written to the bitstream
        for that target. These lists will be read such that element i
        will be the value written the ith time that target was named by a
        :py:class:`Token`.

    Returns
    =======
    bits_past_eof : int
        The number of bits beyond the end of the file which were written.
    """
    bits_past_eof = 0
    
    # The index of the next value to be written into each target array
    next_value_index = {target_identifier: 0 for target_identifier in values}
    
    # Whenever a bounded block is started, the current writer and bits_past_eof
    # count is placed onto the writer_stack and bits_past_eof_stack
    # respectively. At the end of the bounded block, the old values are
    # restored.
    writer_stack = []
    bits_past_eof_stack = []
    try:
        # The latest value to have been written (to send back to the token
        # generator)
        latest_value = None
        while True:
            token = token_generator.send(latest_value)
            
            if token.target is not None:
                latest_value = values[token.target][next_value_index[token.target]]
                next_value_index[token.target] += 1
            else:
                latest_value = 0
            
            if token.type is TokenTypes.nbits:
                if latest_value < 0 or latest_value.bit_length() > token.argument:
                    raise ValueError("Value {} out of range for {}-bit nbits.".format(
                        latest_value, token.argument))
                bits_past_eof += write_bits(writer, token.argument, latest_value)
            elif token.type is TokenTypes.uint:
                if latest_value < 0:
                    raise ValueError("Value {} out of range for uint.".format(
                        latest_value))
                bits_past_eof += write_exp_golomb(writer, latest_value)
            elif token.type is TokenTypes.sint:
                bits_past_eof += write_signed_exp_golomb(writer, latest_value)
            elif token.type is TokenTypes.bounded_block_begin:
                writer_stack.append(writer)
                writer = BoundedWriter(writer, token.argument)
                
                bits_past_eof_stack.append(bits_past_eof)
                bits_past_eof = 0
            elif token.type is TokenTypes.bounded_block_end:
                bits_past_eof = bits_past_eof_stack.pop()
                bits_past_eof += writer.bits_past_eof
                
                # Write any unused bits
                unused_bits = writer.bits_remaining
                writer = writer_stack.pop()
                bits_past_eof += write_bits(writer, unused_bits, latest_value)
            else:
                raise ValueError("Unrecognised TokenType: {}".format(token.type))
    except StopIteration:
        pass
    finally:
        token_generator.close()
    
    # Check for un-closed bounded blocks
    if len(writer_stack) > 0:
        raise ValueError("Not every bounded_block_begin had a matching bounded_block_end")
    
    # Check all values were used
    for target_identifier in values:
        if next_value_index[target_identifier] != len(values[target_identifier]):
            raise ValueError("Targets {} not fully populated".format(
                target_identifier))
    
    return bits_past_eof


def fsm_target_at_offset(token_generator, values, target_offset):
    """
    Determine what value occupies the specified bit offset.
    
    Parameters
    ==========
    token_generator : generator
        A generator which yields :py:class:`Token` tuples defining what values
        are to be written to the bitstream. The generator will be sent the
        written values in return.
    values : {target_identifier: [value, ...], ...}
        A dictionary which, for each of the target identifiers generated by the
        ``token_generator``, contains the values associated with each target.
    target_offset : int
        A target bit-offset into the bitstream at which the target value should
        be found.
    
    Returns
    =======
    (target_identifier, index) or (None, None)
        Returns a 2-tuple which gives the target identifier and index of the
        value which lies at the specified bit offset in the bitstream. The
        identifier and index will be None if the token generator has assigned
        the target identifier None to the token at the specified position.
    
    Raises
    ======
    IndexError
        Raised if the offset is past the end of the bitstream.
    """
    # The index of the next value in each target array
    next_value_index = {target_identifier: 0 for target_identifier in values}
    
    # The current offset into the bitstream
    offset = 0
    
    # A stack of remaining-bit counts for any bounded block we might be in
    bounded_block_remaining_bits = []
    
    try:
        # The latest value to have been written (to send back to the token
        # generator)
        latest_value = None
        while True:
            token = token_generator.send(latest_value)
            
            if token.target is not None:
                latest_value = values[token.target][next_value_index[token.target]]
                next_value_index[token.target] += 1
            else:
                latest_value = 0
            
            current_token_length = 0
            
            if token.type is TokenTypes.nbits:
                current_token_length = token.argument
            elif token.type is TokenTypes.uint:
                current_token_length = exp_golomb_length(latest_value)
            elif token.type is TokenTypes.sint:
                current_token_length = signed_exp_golomb_length(latest_value)
            elif token.type is TokenTypes.bounded_block_begin:
                bounded_block_remaining_bits.append(token.argument)
            elif token.type is TokenTypes.bounded_block_end:
                current_token_length = bounded_block_remaining_bits.pop()
            else:
                raise ValueError("Unrecognised TokenType: {}".format(token.type))
            
            # Remove any part of the current_token_length which is past the end
            # of a containing bounded block
            for i in reversed(range(len(bounded_block_remaining_bits))):
                remaining = bounded_block_remaining_bits[i]
                if remaining <= current_token_length:
                    current_token_length = remaining
                    bounded_block_remaining_bits[i] = 0
                else:
                    bounded_block_remaining_bits[i] -= current_token_length
            
            offset += current_token_length
            
            if offset > target_offset:
                if token.target is not None:
                    return (token.target, next_value_index[token.target] - 1)
                else:
                    return (None, None)
    except StopIteration:
        pass
    finally:
        token_generator.close()
    
    # The offset therefore was past the end of the bitstream; fail!
    raise IndexError("Target offset out of range.")
